{
  "session_id": "25e910b8-16f9-4eaa-83e0-b0cb235ec576",
  "transcript_path": "C:\\Users\\Andrew\\.claude\\projects\\C--Users-Andrew--vscode-RE-analysis-generator\\25e910b8-16f9-4eaa-83e0-b0cb235ec576.jsonl",
  "hook_event_name": "PreToolUse",
  "tool_name": "Write",
  "tool_input": {
    "file_path": "C:\\Users\\Andrew\\.vscode\\RE-analysis-generator\\PRPs\\workflows\\task-07-subtasks-tdd.md",
    "content": "# Task 7: GitHub Actions Workflow - TDD Subtasks\n\n## Overview\nThis document outlines TDD-based subtasks for implementing GitHub Actions workflows, organized by talent/role for optimal task assignment.\n\n## Talent Organization\n\n### 1. DevOps Engineer (Primary)\n**Focus**: CI/CD pipelines, GitHub Actions, deployment automation\n\n### 2. Backend Developer\n**Focus**: Python integration, Epic connections, error handling\n\n### 3. QA Engineer\n**Focus**: Testing workflows, quality gates, monitoring\n\n### 4. Security Engineer\n**Focus**: Secrets management, security scanning, access control\n\n---\n\n## Phase 1: Basic Workflow Setup (Days 1-2)\n\n### Subtask 1.1: GitHub Actions Environment Configuration [DevOps Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_github_environment.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\nimport os\n\nclass TestGitHubEnvironmentSetup:\n    \"\"\"Test GitHub Actions environment configuration.\"\"\"\n    \n    def test_environment_variables_validation(self):\n        \"\"\"Test that required environment variables are validated.\"\"\"\n        # Given: Missing required environment variables\n        with patch.dict(os.environ, {}, clear=True):\n            from phoenix_real_estate.automation.workflows.github_environment import GitHubEnvironmentValidator\n            \n            # When: Validating environment\n            validator = GitHubEnvironmentValidator()\n            \n            # Then: Should identify missing variables\n            missing = validator.get_missing_required_variables()\n            assert \"MONGODB_CONNECTION_STRING\" in missing\n            assert \"MARICOPA_API_KEY\" in missing\n            assert \"TARGET_ZIP_CODES\" in missing\n    \n    def test_github_secrets_loading(self):\n        \"\"\"Test loading configuration from GitHub Secrets.\"\"\"\n        # Given: GitHub Secrets in environment\n        test_env = {\n            \"MONGODB_CONNECTION_STRING\": \"mongodb://test:27017\",\n            \"MARICOPA_API_KEY\": \"test-api-key\",\n            \"TARGET_ZIP_CODES\": \"85031,85033,85035\"\n        }\n        \n        with patch.dict(os.environ, test_env):\n            from phoenix_real_estate.automation.workflows.github_environment import GitHubSecretsLoader\n            \n            # When: Loading secrets\n            loader = GitHubSecretsLoader()\n            config = loader.load_configuration()\n            \n            # Then: Configuration should be properly loaded\n            assert config.get(\"MONGODB_CONNECTION_STRING\") == \"mongodb://test:27017\"\n            assert config.get(\"MARICOPA_API_KEY\") == \"test-api-key\"\n            assert config.get(\"TARGET_ZIP_CODES\") == [\"85031\", \"85033\", \"85035\"]\n    \n    def test_configuration_file_generation(self):\n        \"\"\"Test generation of configuration files for GitHub Actions.\"\"\"\n        # Given: Configuration data\n        config_data = {\n            \"ENVIRONMENT\": \"github_actions\",\n            \"LOG_LEVEL\": \"INFO\",\n            \"WORKFLOW_TIMEOUT_MINUTES\": 75\n        }\n        \n        from phoenix_real_estate.automation.workflows.github_environment import ConfigurationGenerator\n        \n        # When: Generating configuration\n        generator = ConfigurationGenerator()\n        config_path = generator.generate_config_file(config_data)\n        \n        # Then: Configuration file should be created\n        assert config_path.exists()\n        assert config_path.suffix == \".yaml\"\n        # Verify content structure\n        import yaml\n        with open(config_path) as f:\n            loaded_config = yaml.safe_load(f)\n        assert loaded_config[\"environment\"] == \"github_actions\"\n```\n\n**Implementation Tasks**:\n1. Create `GitHubEnvironmentValidator` class\n2. Implement `GitHubSecretsLoader` for secret management\n3. Build `ConfigurationGenerator` for config file creation\n4. Add validation for all required Epic 1-3 configuration\n\n---\n\n### Subtask 1.2: Daily Collection Workflow YAML [DevOps Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_workflow_yaml.py\nimport pytest\nimport yaml\nfrom pathlib import Path\n\nclass TestWorkflowYAML:\n    \"\"\"Test GitHub Actions workflow YAML generation and validation.\"\"\"\n    \n    def test_daily_collection_workflow_structure(self):\n        \"\"\"Test daily collection workflow YAML structure.\"\"\"\n        from phoenix_real_estate.automation.workflows.yaml_generator import DailyCollectionWorkflow\n        \n        # Given: Workflow configuration\n        config = {\n            \"schedule_cron\": \"0 10 * * *\",  # 3 AM Phoenix time\n            \"python_version\": \"3.12.4\",\n            \"timeout_minutes\": 90\n        }\n        \n        # When: Generating workflow YAML\n        workflow = DailyCollectionWorkflow(config)\n        yaml_content = workflow.generate()\n        \n        # Then: YAML should have correct structure\n        parsed = yaml.safe_load(yaml_content)\n        assert parsed[\"name\"] == \"Daily Real Estate Data Collection\"\n        assert parsed[\"on\"][\"schedule\"][0][\"cron\"] == \"0 10 * * *\"\n        assert parsed[\"on\"][\"workflow_dispatch\"] is not None\n        assert parsed[\"jobs\"][\"collect-data\"][\"timeout-minutes\"] == 90\n    \n    def test_workflow_steps_validation(self):\n        \"\"\"Test that workflow contains all required steps.\"\"\"\n        from phoenix_real_estate.automation.workflows.yaml_generator import DailyCollectionWorkflow\n        \n        # When: Generating workflow\n        workflow = DailyCollectionWorkflow({})\n        yaml_content = workflow.generate()\n        parsed = yaml.safe_load(yaml_content)\n        \n        # Then: Should have all required steps\n        steps = parsed[\"jobs\"][\"collect-data\"][\"steps\"]\n        step_names = [step[\"name\"] for step in steps]\n        \n        assert \"Checkout Repository\" in step_names\n        assert \"Set up Python\" in step_names\n        assert \"Install UV Package Manager\" in step_names\n        assert \"Configure Environment\" in step_names\n        assert \"Run Data Collection\" in step_names\n        assert \"Generate Collection Report\" in step_names\n        assert \"Upload Collection Artifacts\" in step_names\n        assert \"Notify on Failure\" in step_names\n    \n    def test_secret_references(self):\n        \"\"\"Test that workflow properly references GitHub Secrets.\"\"\"\n        from phoenix_real_estate.automation.workflows.yaml_generator import DailyCollectionWorkflow\n        \n        # When: Generating workflow\n        workflow = DailyCollectionWorkflow({})\n        yaml_content = workflow.generate()\n        \n        # Then: Should reference secrets correctly\n        assert \"${{ secrets.MONGODB_CONNECTION_STRING }}\" in yaml_content\n        assert \"${{ secrets.MARICOPA_API_KEY }}\" in yaml_content\n        assert \"${{ secrets.WEBSHARE_USERNAME }}\" in yaml_content\n        assert \"${{ secrets.WEBSHARE_PASSWORD }}\" in yaml_content\n```\n\n**Implementation Tasks**:\n1. Create `DailyCollectionWorkflow` YAML generator\n2. Implement proper cron scheduling for Phoenix timezone\n3. Add all required workflow steps with proper ordering\n4. Configure secret references and environment variables\n\n---\n\n### Subtask 1.3: Error Handling and Notifications [Backend Developer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_error_handling.py\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nclass TestWorkflowErrorHandling:\n    \"\"\"Test error handling and notification system.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_workflow_failure_notification(self):\n        \"\"\"Test that workflow failures trigger notifications.\"\"\"\n        from phoenix_real_estate.automation.workflows.notifications import WorkflowNotifier\n        \n        # Given: A workflow failure\n        failure_context = {\n            \"workflow_name\": \"daily-collection\",\n            \"run_id\": \"12345\",\n            \"error\": \"Database connection failed\",\n            \"timestamp\": \"2024-01-15T10:00:00Z\"\n        }\n        \n        # When: Notifying failure\n        notifier = WorkflowNotifier()\n        with patch.object(notifier, '_create_github_issue') as mock_create_issue:\n            await notifier.notify_failure(failure_context)\n        \n        # Then: Should create GitHub issue\n        mock_create_issue.assert_called_once()\n        issue_data = mock_create_issue.call_args[0][0]\n        assert \"Daily Collection Failed\" in issue_data[\"title\"]\n        assert \"12345\" in issue_data[\"body\"]\n        assert \"automation\" in issue_data[\"labels\"]\n        assert \"failure\" in issue_data[\"labels\"]\n    \n    @pytest.mark.asyncio\n    async def test_transient_error_retry(self):\n        \"\"\"Test retry mechanism for transient errors.\"\"\"\n        from phoenix_real_estate.automation.workflows.error_handler import WorkflowErrorHandler\n        \n        # Given: A transient error\n        handler = WorkflowErrorHandler(max_retries=3, retry_delay=1)\n        \n        # Mock function that fails twice then succeeds\n        mock_func = AsyncMock(side_effect=[\n            Exception(\"Network timeout\"),\n            Exception(\"Connection reset\"),\n            {\"success\": True, \"data\": \"collected\"}\n        ])\n        \n        # When: Running with retry\n        result = await handler.run_with_retry(mock_func, \"test_operation\")\n        \n        # Then: Should retry and eventually succeed\n        assert result[\"success\"] is True\n        assert mock_func.call_count == 3\n    \n    @pytest.mark.asyncio\n    async def test_critical_error_handling(self):\n        \"\"\"Test handling of critical non-retryable errors.\"\"\"\n        from phoenix_real_estate.automation.workflows.error_handler import WorkflowErrorHandler\n        \n        # Given: A critical error\n        handler = WorkflowErrorHandler()\n        critical_error = ValueError(\"Invalid configuration\")\n        \n        # When: Handling critical error\n        with pytest.raises(ValueError):\n            await handler.handle_error(critical_error, \"test_operation\", is_critical=True)\n        \n        # Then: Should log and re-raise without retry\n        # Verify logging happened (would need to mock logger)\n```\n\n**Implementation Tasks**:\n1. Create `WorkflowNotifier` for GitHub issue creation\n2. Implement `WorkflowErrorHandler` with retry logic\n3. Add error classification (transient vs critical)\n4. Build comprehensive error reporting\n\n---\n\n## Phase 2: Advanced Orchestration (Days 2-3)\n\n### Subtask 2.1: Epic 3 Orchestration Integration [Backend Developer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_orchestration_integration.py\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nclass TestOrchestrationIntegration:\n    \"\"\"Test integration with Epic 3 orchestration engine.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_orchestration_engine_initialization(self):\n        \"\"\"Test proper initialization of orchestration engine.\"\"\"\n        from phoenix_real_estate.automation.workflows.daily_collection import GitHubActionsDailyCollection\n        \n        # Given: GitHub Actions environment\n        with patch.dict(os.environ, {\"GITHUB_ACTIONS\": \"true\"}):\n            # When: Creating daily collection workflow\n            workflow = GitHubActionsDailyCollection()\n            \n            # Then: Orchestration engine should be initialized\n            assert workflow.orchestration_engine is not None\n            assert workflow.workflow_monitor is not None\n            assert workflow.config.get(\"EXECUTION_ENVIRONMENT\") == \"github_actions\"\n    \n    @pytest.mark.asyncio\n    async def test_workflow_execution_with_orchestration(self):\n        \"\"\"Test workflow execution using orchestration engine.\"\"\"\n        from phoenix_real_estate.automation.workflows.daily_collection import GitHubActionsDailyCollection\n        \n        # Given: Initialized workflow\n        workflow = GitHubActionsDailyCollection()\n        \n        # Mock orchestration engine\n        mock_result = {\n            \"success\": True,\n            \"metrics\": {\n                \"total_properties\": 150,\n                \"collection_duration_seconds\": 3600,\n                \"success_rate\": 0.95\n            }\n        }\n        workflow.orchestration_engine.run_daily_workflow = AsyncMock(return_value=mock_result)\n        \n        # When: Running workflow\n        result = await workflow.run()\n        \n        # Then: Should execute orchestration and return results\n        assert result[\"success\"] is True\n        assert result[\"workflow_result\"][\"metrics\"][\"total_properties\"] == 150\n        assert \"execution_report\" in result\n        assert \"context\" in result\n    \n    @pytest.mark.asyncio\n    async def test_workflow_monitoring_integration(self):\n        \"\"\"Test workflow monitoring and metrics collection.\"\"\"\n        from phoenix_real_estate.automation.workflows.daily_collection import GitHubActionsDailyCollection\n        \n        # Given: Workflow with monitoring\n        workflow = GitHubActionsDailyCollection()\n        workflow.workflow_monitor.on_workflow_started = AsyncMock()\n        workflow.workflow_monitor.on_workflow_completed = AsyncMock()\n        \n        # When: Running workflow\n        workflow.orchestration_engine.run_daily_workflow = AsyncMock(\n            return_value={\"success\": True, \"metrics\": {}}\n        )\n        await workflow.run()\n        \n        # Then: Should track workflow lifecycle\n        workflow.workflow_monitor.on_workflow_started.assert_called_once()\n        workflow.workflow_monitor.on_workflow_completed.assert_called_once()\n        \n        # Verify context passed to monitor\n        start_call = workflow.workflow_monitor.on_workflow_started.call_args\n        assert start_call[0][0] == \"github_actions_daily_collection\"\n        assert \"run_id\" in start_call[0][1]\n```\n\n**Implementation Tasks**:\n1. Create `GitHubActionsDailyCollection` class\n2. Integrate Epic 3 `OrchestrationEngine`\n3. Add `WorkflowMonitor` integration\n4. Implement execution context tracking\n\n---\n\n### Subtask 2.2: Execution Reporting System [QA Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_execution_reporting.py\nimport pytest\nimport json\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\n\nclass TestExecutionReporting:\n    \"\"\"Test execution reporting for GitHub Actions artifacts.\"\"\"\n    \n    def test_execution_report_generation(self):\n        \"\"\"Test generation of execution reports.\"\"\"\n        from phoenix_real_estate.automation.workflows.reporting import ExecutionReportGenerator\n        \n        # Given: Workflow execution results\n        workflow_result = {\n            \"success\": True,\n            \"metrics\": {\n                \"total_properties\": 100,\n                \"success_rate\": 0.92,\n                \"duration_seconds\": 3600\n            },\n            \"collectors\": {\n                \"maricopa\": {\"collected\": 80, \"errors\": 5},\n                \"phoenix_mls\": {\"collected\": 20, \"errors\": 2}\n            }\n        }\n        \n        # When: Generating report\n        generator = ExecutionReportGenerator()\n        report = generator.generate_report(workflow_result)\n        \n        # Then: Report should contain all required sections\n        assert report[\"report_type\"] == \"github_actions_daily_collection\"\n        assert \"execution_time\" in report\n        assert report[\"workflow_result\"] == workflow_result\n        assert \"system_info\" in report\n        assert \"resource_usage\" in report\n    \n    def test_report_file_creation(self):\n        \"\"\"Test that reports are saved correctly for artifacts.\"\"\"\n        from phoenix_real_estate.automation.workflows.reporting import ExecutionReportGenerator\n        \n        # Given: Report generator with temp directory\n        with patch(\"pathlib.Path.mkdir\"):\n            generator = ExecutionReportGenerator()\n            workflow_result = {\"success\": True, \"metrics\": {}}\n            \n            # When: Saving report\n            report_path = generator.save_report(workflow_result, \"daily-collection\")\n            \n            # Then: Report file should be created\n            assert report_path.name.startswith(\"daily-collection-\")\n            assert report_path.suffix == \".json\"\n            assert report_path.parent.name == \"reports\"\n    \n    def test_failure_report_generation(self):\n        \"\"\"Test generation of failure reports with debugging info.\"\"\"\n        from phoenix_real_estate.automation.workflows.reporting import FailureReportGenerator\n        \n        # Given: A workflow failure\n        error = ValueError(\"Database connection failed\")\n        context = {\n            \"run_id\": \"12345\",\n            \"environment\": \"production\",\n            \"timestamp\": \"2024-01-15T10:00:00Z\"\n        }\n        \n        # When: Generating failure report\n        generator = FailureReportGenerator()\n        report = generator.generate_failure_report(error, context)\n        \n        # Then: Report should contain debugging information\n        assert report[\"report_type\"] == \"github_actions_failure\"\n        assert report[\"error_details\"][\"type\"] == \"ValueError\"\n        assert report[\"error_details\"][\"message\"] == \"Database connection failed\"\n        assert \"traceback\" in report[\"error_details\"]\n        assert report[\"context\"] == context\n        assert \"system_state\" in report\n```\n\n**Implementation Tasks**:\n1. Create `ExecutionReportGenerator` class\n2. Implement `FailureReportGenerator` for error reports\n3. Add artifact-friendly JSON formatting\n4. Include system and resource usage metrics\n\n---\n\n### Subtask 2.3: Resource Usage Monitoring [DevOps Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_resource_monitoring.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\n\nclass TestResourceMonitoring:\n    \"\"\"Test resource usage monitoring for GitHub Actions.\"\"\"\n    \n    def test_github_actions_minutes_tracking(self):\n        \"\"\"Test tracking of GitHub Actions minutes usage.\"\"\"\n        from phoenix_real_estate.automation.workflows.monitoring import GitHubActionsResourceMonitor\n        \n        # Given: Resource monitor\n        monitor = GitHubActionsResourceMonitor()\n        \n        # When: Recording workflow execution\n        monitor.start_workflow(\"daily-collection\")\n        # Simulate 5 minute execution\n        with patch(\"time.time\", side_effect=[0, 300]):\n            monitor.end_workflow(\"daily-collection\")\n        \n        # Then: Should track minutes used\n        usage = monitor.get_minutes_used()\n        assert usage[\"daily-collection\"] == 5.0\n        assert monitor.get_total_minutes_used() == 5.0\n    \n    def test_memory_usage_monitoring(self):\n        \"\"\"Test memory usage monitoring during execution.\"\"\"\n        from phoenix_real_estate.automation.workflows.monitoring import MemoryMonitor\n        \n        # Given: Memory monitor\n        monitor = MemoryMonitor()\n        \n        # Mock psutil\n        with patch(\"psutil.Process\") as mock_process:\n            mock_process.return_value.memory_info.return_value.rss = 512 * 1024 * 1024  # 512MB\n            \n            # When: Getting memory usage\n            usage_mb = monitor.get_current_usage_mb()\n            \n            # Then: Should return usage in MB\n            assert usage_mb == 512.0\n    \n    def test_resource_limits_enforcement(self):\n        \"\"\"Test enforcement of resource limits.\"\"\"\n        from phoenix_real_estate.automation.workflows.monitoring import ResourceLimitEnforcer\n        \n        # Given: Resource limits\n        limits = {\n            \"max_memory_mb\": 1024,\n            \"max_minutes\": 90,\n            \"max_artifact_size_mb\": 100\n        }\n        \n        enforcer = ResourceLimitEnforcer(limits)\n        \n        # When: Checking limits\n        # Memory within limit\n        assert enforcer.check_memory_limit(800) is True\n        assert enforcer.check_memory_limit(1200) is False\n        \n        # Minutes within limit\n        assert enforcer.check_minutes_limit(60) is True\n        assert enforcer.check_minutes_limit(95) is False\n```\n\n**Implementation Tasks**:\n1. Create `GitHubActionsResourceMonitor` for minute tracking\n2. Implement `MemoryMonitor` for memory usage\n3. Build `ResourceLimitEnforcer` for limit checking\n4. Add resource usage to execution reports\n\n---\n\n## Phase 3: Quality Assurance (Days 3-4)\n\n### Subtask 3.1: Quality Assurance Workflow [QA Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_qa_workflow.py\nimport pytest\nimport yaml\nfrom unittest.mock import patch, MagicMock\n\nclass TestQAWorkflow:\n    \"\"\"Test quality assurance workflow implementation.\"\"\"\n    \n    def test_qa_workflow_yaml_structure(self):\n        \"\"\"Test QA workflow YAML generation.\"\"\"\n        from phoenix_real_estate.automation.workflows.yaml_generator import QAWorkflow\n        \n        # Given: QA workflow configuration\n        config = {\n            \"schedule_cron\": \"0 2 * * 0\",  # Weekly Sunday 2 AM UTC\n            \"environments\": [\"development\", \"staging\"],\n            \"python_version\": \"3.12.4\"\n        }\n        \n        # When: Generating QA workflow\n        workflow = QAWorkflow(config)\n        yaml_content = workflow.generate()\n        parsed = yaml.safe_load(yaml_content)\n        \n        # Then: Should have matrix strategy\n        assert parsed[\"name\"] == \"Quality Assurance and Integration Tests\"\n        assert parsed[\"jobs\"][\"integration-tests\"][\"strategy\"][\"matrix\"][\"environment\"] == [\"development\", \"staging\"]\n        assert \"Epic 1-2-3 Integration Test\" in [step[\"name\"] for step in parsed[\"jobs\"][\"integration-tests\"][\"steps\"]]\n    \n    @pytest.mark.asyncio\n    async def test_integration_test_execution(self):\n        \"\"\"Test execution of integration tests.\"\"\"\n        from phoenix_real_estate.automation.workflows.qa_runner import IntegrationTestRunner\n        \n        # Given: Test runner\n        runner = IntegrationTestRunner(environment=\"staging\")\n        \n        # Mock test execution\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.return_value.returncode = 0\n            mock_run.return_value.stdout = \"All tests passed\"\n            \n            # When: Running integration tests\n            result = await runner.run_epic_integration_tests()\n            \n            # Then: Should execute pytest with correct parameters\n            assert result[\"success\"] is True\n            mock_run.assert_called()\n            call_args = mock_run.call_args[0][0]\n            assert \"pytest\" in call_args\n            assert \"tests/integration/\" in call_args\n            assert \"--cov=phoenix_real_estate\" in call_args\n    \n    def test_performance_benchmarking(self):\n        \"\"\"Test performance benchmark execution.\"\"\"\n        from phoenix_real_estate.automation.workflows.benchmarks import PerformanceBenchmark\n        \n        # Given: Performance benchmark\n        benchmark = PerformanceBenchmark()\n        \n        # Mock benchmark results\n        mock_results = {\n            \"collection_speed\": {\n                \"properties_per_minute\": 50,\n                \"average_response_time_ms\": 200\n            },\n            \"resource_usage\": {\n                \"peak_memory_mb\": 512,\n                \"cpu_usage_percent\": 45\n            }\n        }\n        \n        with patch.object(benchmark, '_run_benchmark', return_value=mock_results):\n            # When: Running benchmarks\n            results = benchmark.run()\n            \n            # Then: Should return performance metrics\n            assert results[\"collection_speed\"][\"properties_per_minute\"] == 50\n            assert results[\"resource_usage\"][\"peak_memory_mb\"] == 512\n```\n\n**Implementation Tasks**:\n1. Create `QAWorkflow` YAML generator\n2. Implement `IntegrationTestRunner` for Epic integration tests\n3. Build `PerformanceBenchmark` for performance testing\n4. Add test result aggregation and reporting\n\n---\n\n### Subtask 3.2: Security Scanning Integration [Security Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_security_scanning.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\n\nclass TestSecurityScanning:\n    \"\"\"Test security scanning integration.\"\"\"\n    \n    def test_bandit_security_scan(self):\n        \"\"\"Test Bandit security scanning integration.\"\"\"\n        from phoenix_real_estate.automation.workflows.security import SecurityScanner\n        \n        # Given: Security scanner\n        scanner = SecurityScanner()\n        \n        # Mock bandit execution\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.return_value.returncode = 0\n            mock_run.return_value.stdout = json.dumps({\n                \"errors\": [],\n                \"results\": [],\n                \"metrics\": {\"high\": 0, \"medium\": 0, \"low\": 0}\n            })\n            \n            # When: Running security scan\n            results = scanner.run_bandit_scan(\"src/\")\n            \n            # Then: Should execute bandit and parse results\n            assert results[\"passed\"] is True\n            assert results[\"high_severity_issues\"] == 0\n            mock_run.assert_called()\n            assert \"-r src/\" in \" \".join(mock_run.call_args[0][0])\n    \n    def test_dependency_vulnerability_scan(self):\n        \"\"\"Test dependency vulnerability scanning.\"\"\"\n        from phoenix_real_estate.automation.workflows.security import DependencyScanner\n        \n        # Given: Dependency scanner\n        scanner = DependencyScanner()\n        \n        # Mock safety check\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.return_value.returncode = 0\n            mock_run.return_value.stdout = \"No known security vulnerabilities found\"\n            \n            # When: Scanning dependencies\n            results = scanner.scan_dependencies()\n            \n            # Then: Should run safety check\n            assert results[\"vulnerabilities_found\"] == 0\n            assert results[\"scan_passed\"] is True\n    \n    def test_secrets_detection(self):\n        \"\"\"Test detection of exposed secrets.\"\"\"\n        from phoenix_real_estate.automation.workflows.security import SecretsDetector\n        \n        # Given: Secrets detector\n        detector = SecretsDetector()\n        \n        # Test file content with potential secret\n        test_content = '''\n        API_KEY = \"sk-1234567890abcdef\"\n        DATABASE_URL = \"mongodb://user:pass@localhost\"\n        '''\n        \n        # When: Scanning for secrets\n        with patch(\"builtins.open\", mock_open(read_data=test_content)):\n            results = detector.scan_file(\"config.py\")\n        \n        # Then: Should detect potential secrets\n        assert len(results[\"potential_secrets\"]) >= 1\n        assert any(\"API_KEY\" in str(secret) for secret in results[\"potential_secrets\"])\n```\n\n**Implementation Tasks**:\n1. Create `SecurityScanner` for Bandit integration\n2. Implement `DependencyScanner` for vulnerability checking\n3. Build `SecretsDetector` for secret detection\n4. Add security report generation\n\n---\n\n### Subtask 3.3: QA Report Generation [QA Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_qa_reporting.py\nimport pytest\nfrom pathlib import Path\nimport json\n\nclass TestQAReporting:\n    \"\"\"Test QA report generation.\"\"\"\n    \n    def test_qa_summary_report_generation(self):\n        \"\"\"Test generation of comprehensive QA summary.\"\"\"\n        from phoenix_real_estate.automation.workflows.reporting import QASummaryGenerator\n        \n        # Given: QA test results\n        test_results = {\n            \"unit_tests\": {\n                \"passed\": 150,\n                \"failed\": 2,\n                \"coverage\": 92.5\n            },\n            \"integration_tests\": {\n                \"passed\": 25,\n                \"failed\": 0,\n                \"duration_seconds\": 300\n            },\n            \"performance_benchmarks\": {\n                \"collection_speed\": 50,\n                \"memory_usage_mb\": 512\n            },\n            \"security_scan\": {\n                \"vulnerabilities\": 0,\n                \"code_issues\": 3\n            }\n        }\n        \n        # When: Generating QA summary\n        generator = QASummaryGenerator()\n        summary = generator.generate_summary(test_results)\n        \n        # Then: Summary should aggregate all results\n        assert summary[\"overall_status\"] == \"PASSED_WITH_WARNINGS\"  # Due to 2 unit test failures\n        assert summary[\"test_coverage\"] == 92.5\n        assert summary[\"total_tests_run\"] == 177\n        assert summary[\"quality_score\"] >= 0.9  # High score despite minor issues\n    \n    def test_qa_artifact_packaging(self):\n        \"\"\"Test packaging of QA artifacts for upload.\"\"\"\n        from phoenix_real_estate.automation.workflows.reporting import QAArtifactPackager\n        \n        # Given: Various QA reports\n        reports = {\n            \"qa_summary.json\": {\"status\": \"passed\"},\n            \"performance_report.json\": {\"metrics\": {}},\n            \"security_report.json\": {\"vulnerabilities\": 0}\n        }\n        \n        # When: Packaging artifacts\n        packager = QAArtifactPackager()\n        artifact_dir = packager.package_reports(reports, \"qa-run-123\")\n        \n        # Then: All reports should be in artifact directory\n        assert artifact_dir.exists()\n        assert (artifact_dir / \"qa_summary.json\").exists()\n        assert (artifact_dir / \"performance_report.json\").exists()\n        assert (artifact_dir / \"security_report.json\").exists()\n```\n\n**Implementation Tasks**:\n1. Create `QASummaryGenerator` for report aggregation\n2. Implement `QAArtifactPackager` for artifact preparation\n3. Add quality scoring algorithm\n4. Build comprehensive QA dashboard data\n\n---\n\n## Phase 4: Production Hardening (Days 4-5)\n\n### Subtask 4.1: Container Build Optimization [DevOps Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_container_optimization.py\nimport pytest\nfrom unittest.mock import patch, MagicMock\n\nclass TestContainerOptimization:\n    \"\"\"Test Docker container optimization.\"\"\"\n    \n    def test_dockerfile_generation(self):\n        \"\"\"Test optimized Dockerfile generation.\"\"\"\n        from phoenix_real_estate.automation.workflows.docker import DockerfileGenerator\n        \n        # Given: Container configuration\n        config = {\n            \"python_version\": \"3.12.4\",\n            \"base_image\": \"python:3.12.4-slim\",\n            \"optimize_size\": True\n        }\n        \n        # When: Generating Dockerfile\n        generator = DockerfileGenerator(config)\n        dockerfile_content = generator.generate()\n        \n        # Then: Should use multi-stage build\n        assert \"FROM python:3.12.4-slim AS builder\" in dockerfile_content\n        assert \"FROM python:3.12.4-slim\" in dockerfile_content\n        assert \"COPY --from=builder\" in dockerfile_content\n        assert \"RUN pip install --no-cache-dir\" in dockerfile_content\n    \n    def test_container_size_validation(self):\n        \"\"\"Test container size stays under 500MB limit.\"\"\"\n        from phoenix_real_estate.automation.workflows.docker import ContainerSizeValidator\n        \n        # Given: Built container\n        validator = ContainerSizeValidator()\n        \n        # Mock docker image inspect\n        with patch(\"subprocess.run\") as mock_run:\n            mock_run.return_value.stdout = json.dumps({\n                \"Size\": 450 * 1024 * 1024  # 450MB\n            })\n            \n            # When: Validating size\n            result = validator.validate_image_size(\"phoenix-real-estate:latest\")\n            \n            # Then: Should pass validation\n            assert result[\"passed\"] is True\n            assert result[\"size_mb\"] == 450\n            assert result[\"size_mb\"] < 500  # Under limit\n    \n    def test_layer_caching_configuration(self):\n        \"\"\"Test Docker layer caching setup.\"\"\"\n        from phoenix_real_estate.automation.workflows.yaml_generator import BuildWorkflow\n        \n        # When: Generating build workflow\n        workflow = BuildWorkflow({})\n        yaml_content = workflow.generate()\n        parsed = yaml.safe_load(yaml_content)\n        \n        # Then: Should configure layer caching\n        build_step = next(\n            step for step in parsed[\"jobs\"][\"build\"][\"steps\"] \n            if step[\"name\"] == \"Build and Push Docker Image\"\n        )\n        assert build_step[\"with\"][\"cache-from\"] == \"type=gha\"\n        assert build_step[\"with\"][\"cache-to\"] == \"type=gha,mode=max\"\n```\n\n**Implementation Tasks**:\n1. Create `DockerfileGenerator` with multi-stage builds\n2. Implement `ContainerSizeValidator` for size limits\n3. Configure GitHub Actions caching\n4. Add container security scanning\n\n---\n\n### Subtask 4.2: Deployment Workflow [DevOps Engineer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_deployment.py\nimport pytest\nfrom unittest.mock import patch, AsyncMock\n\nclass TestDeploymentWorkflow:\n    \"\"\"Test deployment workflow implementation.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_production_deployment_gates(self):\n        \"\"\"Test production deployment requires approval.\"\"\"\n        from phoenix_real_estate.automation.workflows.deployment import DeploymentManager\n        \n        # Given: Deployment manager\n        manager = DeploymentManager(environment=\"production\")\n        \n        # When: Checking deployment requirements\n        requirements = await manager.check_deployment_requirements()\n        \n        # Then: Should require specific conditions\n        assert requirements[\"requires_approval\"] is True\n        assert requirements[\"required_branch\"] == \"main\"\n        assert requirements[\"required_tests_passed\"] is True\n    \n    @pytest.mark.asyncio\n    async def test_deployment_rollback_capability(self):\n        \"\"\"Test deployment rollback mechanism.\"\"\"\n        from phoenix_real_estate.automation.workflows.deployment import DeploymentManager\n        \n        # Given: Failed deployment\n        manager = DeploymentManager(environment=\"production\")\n        deployment_id = \"deploy-123\"\n        \n        # Mock deployment failure\n        with patch.object(manager, '_deploy_image') as mock_deploy:\n            mock_deploy.side_effect = Exception(\"Deployment failed\")\n            \n            # When: Deployment fails\n            try:\n                await manager.deploy(deployment_id)\n            except Exception:\n                # Then: Should trigger rollback\n                rollback_result = await manager.rollback(deployment_id)\n                assert rollback_result[\"success\"] is True\n                assert rollback_result[\"rolled_back_to\"] == \"previous-version\"\n    \n    def test_deployment_metadata_tagging(self):\n        \"\"\"Test deployment metadata and tagging.\"\"\"\n        from phoenix_real_estate.automation.workflows.deployment import DeploymentTagger\n        \n        # Given: Deployment context\n        context = {\n            \"commit_sha\": \"abc123\",\n            \"build_number\": \"456\",\n            \"timestamp\": \"2024-01-15T10:00:00Z\",\n            \"triggered_by\": \"schedule\"\n        }\n        \n        # When: Creating deployment tags\n        tagger = DeploymentTagger()\n        tags = tagger.create_deployment_tags(context)\n        \n        # Then: Should create comprehensive tags\n        assert \"v1.0.456\" in tags\n        assert \"latest\" in tags\n        assert \"sha-abc123\" in tags\n        assert len(tags) >= 3\n```\n\n**Implementation Tasks**:\n1. Create `DeploymentManager` with environment gates\n2. Implement rollback capability\n3. Build `DeploymentTagger` for version management\n4. Add deployment verification checks\n\n---\n\n### Subtask 4.3: Operational Documentation [Backend Developer]\n\n**TDD Test First**:\n```python\n# tests/automation/workflows/test_documentation.py\nimport pytest\nfrom pathlib import Path\n\nclass TestOperationalDocumentation:\n    \"\"\"Test operational documentation generation.\"\"\"\n    \n    def test_runbook_generation(self):\n        \"\"\"Test generation of operational runbooks.\"\"\"\n        from phoenix_real_estate.automation.workflows.documentation import RunbookGenerator\n        \n        # Given: System configuration and workflows\n        config = {\n            \"workflows\": [\"daily-collection\", \"build-deploy\", \"qa\"],\n            \"environments\": [\"development\", \"staging\", \"production\"],\n            \"monitoring_endpoints\": [\"metrics\", \"health\", \"status\"]\n        }\n        \n        # When: Generating runbook\n        generator = RunbookGenerator()\n        runbook = generator.generate_runbook(config)\n        \n        # Then: Runbook should contain operational procedures\n        assert \"Daily Collection Workflow\" in runbook\n        assert \"Troubleshooting Guide\" in runbook\n        assert \"Rollback Procedures\" in runbook\n        assert \"Monitoring and Alerts\" in runbook\n        assert \"Emergency Contacts\" in runbook\n    \n    def test_secrets_documentation(self):\n        \"\"\"Test documentation of required secrets.\"\"\"\n        from phoenix_real_estate.automation.workflows.documentation import SecretsDocumentationGenerator\n        \n        # Given: Required secrets configuration\n        secrets_config = {\n            \"MONGODB_CONNECTION_STRING\": {\n                \"description\": \"MongoDB connection string\",\n                \"format\": \"mongodb://user:pass@host:port/db\",\n                \"required\": True\n            },\n            \"MARICOPA_API_KEY\": {\n                \"description\": \"Maricopa County API key\",\n                \"format\": \"alphanumeric string\",\n                \"required\": True\n            }\n        }\n        \n        # When: Generating documentation\n        generator = SecretsDocumentationGenerator()\n        docs = generator.generate_secrets_guide(secrets_config)\n        \n        # Then: Should document all secrets\n        assert \"## Required GitHub Secrets\" in docs\n        assert \"MONGODB_CONNECTION_STRING\" in docs\n        assert \"mongodb://user:pass@host:port/db\" in docs\n        assert \"### How to Configure\" in docs\n    \n    def test_monitoring_guide_generation(self):\n        \"\"\"Test generation of monitoring guide.\"\"\"\n        from phoenix_real_estate.automation.workflows.documentation import MonitoringGuideGenerator\n        \n        # Given: Monitoring configuration\n        monitoring_config = {\n            \"metrics\": [\n                \"workflow_execution_time\",\n                \"collection_success_rate\",\n                \"github_actions_minutes_used\"\n            ],\n            \"alerts\": [\n                {\"name\": \"workflow_failure\", \"threshold\": \"3 consecutive failures\"},\n                {\"name\": \"high_resource_usage\", \"threshold\": \"> 80% limit\"}\n            ]\n        }\n        \n        # When: Generating monitoring guide\n        generator = MonitoringGuideGenerator()\n        guide = generator.generate_guide(monitoring_config)\n        \n        # Then: Should include monitoring procedures\n        assert \"## Key Metrics\" in guide\n        assert \"workflow_execution_time\" in guide\n        assert \"## Alert Configuration\" in guide\n        assert \"workflow_failure\" in guide\n```\n\n**Implementation Tasks**:\n1. Create `RunbookGenerator` for operational procedures\n2. Implement `SecretsDocumentationGenerator`\n3. Build `MonitoringGuideGenerator`\n4. Generate comprehensive ops documentation\n\n---\n\n## Summary\n\n### Task Distribution by Role\n\n**DevOps Engineer (Primary)**:\n- Subtask 1.1: GitHub Actions Environment Configuration\n- Subtask 1.2: Daily Collection Workflow YAML\n- Subtask 2.3: Resource Usage Monitoring\n- Subtask 4.1: Container Build Optimization\n- Subtask 4.2: Deployment Workflow\n\n**Backend Developer**:\n- Subtask 1.3: Error Handling and Notifications\n- Subtask 2.1: Epic 3 Orchestration Integration\n- Subtask 4.3: Operational Documentation\n\n**QA Engineer**:\n- Subtask 2.2: Execution Reporting System\n- Subtask 3.1: Quality Assurance Workflow\n- Subtask 3.3: QA Report Generation\n\n**Security Engineer**:\n- Subtask 3.2: Security Scanning Integration\n\n### TDD Implementation Strategy\n\n1. **Write Tests First**: Each subtask starts with comprehensive test cases\n2. **Test Categories**:\n   - Unit tests for individual components\n   - Integration tests for Epic connections\n   - System tests for complete workflows\n3. **Coverage Requirements**: Minimum 90% for all new code\n4. **Continuous Validation**: Tests run on every commit via GitHub Actions\n\n### Success Metrics\n\n- All tests passing (100%)\n- Code coverage > 90%\n- Workflows execute within resource limits\n- Documentation complete and validated\n- Security scans passing\n- Performance benchmarks met"
  }
}